---
title: "STAT 425 Spring 2016 Sections 1UG, 1GR"
author: "Ruize Chen"
date: "February 19, 2016"
output: html_document
---

## Exercise 1

* **Using the stopping data set (from package alr4), fit a simple linear regression model with
the stopping distance as the response and speed as the predictor.**

```{r}
library(alr4)
fit0<-lm(Distance~Speed,data=stopping)
```

* **(a) [2 pts] Produce the four default diagnostic plots given by R.**

```{r, fig.width=8.27, fig.height=8.27}
par(mfrow=c(2,2))
plot(fit0,add.smooth=FALSE)
```

* **(b) [2 pts] Using an appropriate diagnostic plot, check whether the assumed mean function is appropriate.**

**In order to examine whether the assumed mean function is appropriate, we need to check the residuals v.s. fitted plot.**

```{r}
plot(fit0, which = 1:1)
```

**By examing the plot we conclude that the assumed mean function is inappropriate because:**

* **There is an obvious curvature pattern/a clear shape of the residuals.**

* **There are a lot of outliers.**

* **The residual values are very big.**

**(c) [2 pts] Check the assumption of constant variance.**

```{r}
plot(fit0, which = 3:3)
```

**By examing the plot we conclude that the the assumption of homoscedasticity is violated because:**

* **There is an obvious curvature pattern/a clear shape of the residuals.**

**(d) [2 pts] What is the largest (most positive) least-squares residual value? What is the smallest (most negative) least-squares residual value?**

```{r}
#The largest (most positive) least-squares residual value is:
max(fit0$residuals)
#The smallest (most negative) least-squares residual value is:
min(fit0$residuals)
```

**(e) [2 pts] Identify one observation that has the largest leverage value.**

```{r}
c(which.max(hatvalues(fit0)),max(hatvalues(fit0)))
plot(hatvalues(fit0), type="h", main="Leverages")
points(which.max(hatvalues(fit0)),max(hatvalues(fit0)),col="red",pch=19)
abline(v=which.max(hatvalues(fit0)), h=max(hatvalues(fit0)),col="blue", lty=3)
```


**(f) [2 pts] Check for outliers. (You should use diagnostic plots | a formal test is not necessary.)**

* **From the above diagnostic plots we can see that observation #55, #60, and #62 are tagged as outliers.**

**(g) [2 pts] Check for inuential points.**

```{r fig.width=8.27, fig.height=8.27}
influencePlot(fit0,main="Influence Plot" )
```

```{r}
plot(cooks.distance(fit0), type="h",main="Cooks Distance Plot")
```


#Exercise 2.

**[14 pts] Using the drugcost data set (from package alr4), fit a model with COST as the response and all of the other variables as predictors. Then answer the same parts as in Problem 1.**

```{r}
library(alr4)
fit1<-lm(COST~RXPM+GS+RI+COPAY+AGE+F,data=drugcost)
```

**(a) [2 pts] Produce the four default diagnostic plots given by R**

```{r fig.width=8.27, fig.height=8.27}
par(mfrow=c(2,2))
plot(fit1,add.smooth=FALSE)
```

* **(b) [2 pts] Using an appropriate diagnostic plot, check whether the assumed mean function is appropriate.**

**In order to examine whether the assumed mean function is appropriate, we need to check the residuals v.s. fitted plot.**

```{r}
plot(fit1, which = 1:1)
```

**By examing the plot we conclude that the assumed mean function is appropriate because:**

* **There is no obvious curvature pattern of the residuals.**

* **There are symmetrically distributed, tending to cluster towards the middle of the plot.**

* **The residual values are relatively very small.**

**(c) [2 pts] Check the assumption of constant variance.**

```{r}
plot(fit1, which = 3:3)
```

**By examing the plot we conclude that the the assumption of homoscedasticity is violated because:**

* **The trend is roughly flat**

**(d) [2 pts] What is the largest (most positive) least-squares residual value? What is the smallest (most negative) least-squares residual value?**

```{r}
#The largest (most positive) least-squares residual value is:
max(fit1$residuals)
#The smallest (most negative) least-squares residual value is:
min(fit1$residuals)
```

**(e) [2 pts] Identify one observation that has the largest leverage value.**

```{r}
c(which.max(hatvalues(fit1)),max(hatvalues(fit1)))
plot(hatvalues(fit1), type="h", main="Leverages")
points(which.max(hatvalues(fit1)),max(hatvalues(fit1)),col="red",pch=19)
abline(v=which.max(hatvalues(fit1)), h=max(hatvalues(fit1)),col="blue", lty=3)
```


**(f) [2 pts] Check for outliers. (You should use diagnostic plots | a formal test is not necessary.)**

* **According to the residuals v.s. fitted plot we can see that observation NJ, W_PA, GA2, are tagged as outliers.**

**(g) [2 pts] Check for inuential points.**

```{r fig.width=8.27, fig.height=8.27}
influencePlot(fit1,main="Influence Plot" )
```

```{r}
plot(cooks.distance(fit1), type="h",main="Cooks Distance Plot")
```

# Exercise 3.

**Using the fuel2001 data set (from package alr4), fit a model with FuelC as the response and Tax, Drivers, and Income as predictors. Answer the following.**

```{r}
fit2<-lm(FuelC~Tax+Drivers+Income, data=fuel2001)
```

**(a) [2 pts] Produce a plot of the standardized residuals ri versus the ordinary (least squares) residuals ^ei. (Show R code.)**

```{r}
plot(resid(fit2),rstandard(fit2))
```

**(b) [2 pts] The points in this plot do not exactly fall on a straight line. Briefly explain why. [ Hint: What is the formula for the standardized residuals? ]**

* **Because the variance of ordinary residuals is not constant.**

**(c) [2 pts] List the studentized residuals ti (which are used as test statistics in the Mean Shift Test).**

```{r}
rstudent(fit2)
```

**(d) [2 pts] Perform all Mean Shift Tests without Bonferroni adjustment, using alpha = 0.05. Which states are identified as outliers?**

```{r}
critval<-qt(0.05/2, df=df.residual(fit2)-1, lower=FALSE)
critval
which(abs(rstudent(fit2)) > critval)
```

**(e) [2 pts] Perform all Mean Shift Tests with Bonferroni adjustment, using alpha = 0:05. Which states are identified as outliers?**

```{r}
critval <- qt(0.05/(2*nobs(fit2)), df=df.residual(fit2)-1, lower=FALSE)
critval
which(abs(rstudent(fit2)) > critval)
```

# Exercise 4. 

**Using R, produce a grid of 9 normal probability plots (qqnorm) for samples of size n = 50 simulated independently from a geometric distribution with parameter prob equal to 0.4.**

**(a) [2 pts] Display your plots and the R code you used to produce them.**

```{r fig.width=8.27, fig.height=8.27}
par(pty="s")
par(mfrow=c(3,3)) 
for(i in 1:9){
qqnorm(rgeom(50,prob=0.4))
}
```

**(b) [2 pts] Describe two distinct ways in which these plots tend to differ in appearance from what you would expect for normally-distributed data.**

* **(1)Geometric data plots appear to be step-wise because geometric distribution is discrete. However normally distributed data will appear to be continuous.**

* **(2)The geometric data is heavily skewed at lower values while normal data is unskewed.**

